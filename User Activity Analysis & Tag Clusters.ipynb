{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            print(\"Data loaded successfully from\", file_name)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file does not exist.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: The file is not a valid JSON.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(\"./socialmaps-items.json\")\n",
    "items = data[\"items\"]\n",
    "\n",
    "ITEMS_BY_ID = {item['id']: item for item in items}\n",
    "assert len(items) == len(ITEMS_BY_ID)\n",
    "len(ITEMS_BY_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_tags(item_list):\n",
    "    key = \"tags\"\n",
    "    unique_values = set()\n",
    "    for item in item_list:\n",
    "        if key in item and isinstance(item[key], list):  # Ensure it's a list\n",
    "            for tag in item[key]:\n",
    "                unique_values.add(tag)\n",
    "    return unique_values\n",
    "\n",
    "unique_tags = find_unique_tags(items)\n",
    "\n",
    "print(unique_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_primarytopics(item_list):\n",
    "    key = \"primaryTopic\"\n",
    "    unique_values = set()\n",
    "    for item in item_list:\n",
    "        if key in item and isinstance(item[key], str):  # Ensure it's a string\n",
    "            unique_values.add(item[key])\n",
    "    return unique_values\n",
    "\n",
    "unique_primarytopics = find_unique_primarytopics(items)\n",
    "print(unique_primarytopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "## the below will aggregate all JSON exports from:\n",
    "# search: https://socialmap-berlin.de/report/search/YYYY-MM\n",
    "# browse: https://socialmap-berlin.de/report/item/YYYY-MM\n",
    "# (subsequent processing will correctly interpret searc hand browsing behavior)\n",
    "\n",
    "pattern = re.compile(r'^goaccess-\\d+\\.json$')\n",
    "\n",
    "json_objects = []\n",
    "\n",
    "files_in_dir = os.listdir()\n",
    "\n",
    "for filename in files_in_dir:\n",
    "    if pattern.match(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            json_content = json.load(file)\n",
    "            print(filename)\n",
    "            json_objects.append(json_content)\n",
    "\n",
    "\n",
    "len(json_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "class LocaleParserInfo(parser.parserinfo):\n",
    "        MONTHS = [('Jan', 'Januar', 'January', 'Jänner'),\n",
    "                  ('Feb', 'Februar', 'February'),\n",
    "                  ('Mrz', 'März', 'March', 'Mar', 'Mär'),\n",
    "                  ('Apr', 'April'),\n",
    "                  ('Mai', 'May'),\n",
    "                  ('Jun', 'Juni', 'June'),\n",
    "                  ('Jul', 'Juli', 'July'),\n",
    "                  ('Aug', 'August'),\n",
    "                  ('Sep', 'September'),\n",
    "                  ('Okt', 'Oktober', 'October', 'Oct', 'Okt'),\n",
    "                  ('Nov', 'November'),\n",
    "                  ('Dez', 'Dezember', 'Dec', 'December')]\n",
    "\n",
    "def process_requesturls_file(json_obj_, cumulative_values_):\n",
    "    start_date = parser.parse(json_obj_[\"general\"][\"start_date\"], dayfirst=True, parserinfo=LocaleParserInfo())\n",
    "    end_date = parser.parse(json_obj_[\"general\"][\"end_date\"], dayfirst=True, parserinfo=LocaleParserInfo())\n",
    "    \n",
    "    key = \"%s\"%(start_date.strftime(\"%Y-%m\"))\n",
    "    values = dict()\n",
    "    summary = dict(search=0, browse=0, suggest=0)\n",
    "    for r in json_obj_[\"requests\"][\"data\"]:\n",
    "        \n",
    "        n_hits = int(r[\"hits\"][\"count\"])\n",
    "        \n",
    "        if r[\"data\"].startswith(\"/search/\"):\n",
    "            searchterm = r[\"data\"][len(\"/search/\"):]\n",
    "            \n",
    "            values[searchterm] = values.get(searchterm, 0) + n_hits\n",
    "            summary[\"search\"] += n_hits\n",
    "            cumulative_values_[\"search\"][searchterm] = cumulative_values_[\"search\"].get(searchterm, 0) + n_hits\n",
    "        \n",
    "        elif r[\"data\"].startswith(\"/item/\"):\n",
    "            item = r[\"data\"][len(\"/item/\"):]\n",
    "            \n",
    "            if item.startswith(\"new_\"):\n",
    "                summary[\"suggest\"] += n_hits\n",
    "                \n",
    "            else:\n",
    "                values[item] = values.get(item, 0) + n_hits\n",
    "                summary[\"browse\"] += n_hits\n",
    "                cumulative_values_[\"browse\"][item] = cumulative_values_[\"browse\"].get(item, 0) + n_hits\n",
    "\n",
    "        else:\n",
    "            print(\"??\", r)\n",
    "            return None, None, None\n",
    "    return key, values, summary\n",
    "\n",
    "cumulative_values = dict(search=dict(), browse=dict())\n",
    "\n",
    "vega_summary_data = []\n",
    "\n",
    "for jo in json_objects:\n",
    "    timeframe, values, summary = process_requesturls_file(jo, cumulative_values)\n",
    "    \n",
    "    vega_summary_data.append(dict(\n",
    "        month=timeframe,\n",
    "        user_journey=\"search\" if summary[\"search\"] > 0 else \"browse\",\n",
    "        hits=summary[\"search\"] if summary[\"search\"] > 0 else summary[\"browse\"],\n",
    "    ))\n",
    "    if summary[\"browse\"] > 0:\n",
    "        vega_summary_data.append(dict(\n",
    "            month=timeframe,\n",
    "            user_journey=\"suggest\",\n",
    "            hits=summary[\"suggest\"]\n",
    "        ))\n",
    "    print(timeframe, summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in cumulative_values[\"browse\"]:\n",
    "    if id_ == \"9749276938bd792e\":\n",
    "        print(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "alt.Chart(pd.DataFrame(vega_summary_data), width=600).mark_line().encode(\n",
    "    x='month:T',\n",
    "    color='user_journey',\n",
    "    y='hits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alt.Chart(pd.DataFrame(vega_summary_data), width=200).mark_boxplot().encode(\n",
    "    x='user_journey',\n",
    "    color='user_journey',\n",
    "    y='hits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_hits = []\n",
    "zero = 0\n",
    "for item_id, item in ITEMS_BY_ID.items():\n",
    "    # note that not all hits correspond to active entries\n",
    "    if item_id in cumulative_values[\"browse\"]:\n",
    "        item_hits.append(dict(value=cumulative_values[\"browse\"].get(item_id, 0)))\n",
    "    else:\n",
    "        zero += 1\n",
    "\n",
    "chart = alt.Chart(pd.DataFrame(item_hits), width=800, title=\"From 2022-09 until 2024-03, only entries with at least 1 hit. {}% entries not represented (no hits) of {} total\".format(\n",
    "    round(zero/len(ITEMS_BY_ID) * 100), len(ITEMS_BY_ID))\n",
    "                 ).mark_bar().encode(\n",
    "    alt.X('value', bin=alt.Bin(maxbins=100)),\n",
    "    y=alt.Y('count()', title='Number of entries with this many hits')\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = dict()\n",
    "websites_w_mx_topics = set()\n",
    "\n",
    "# website or title duplicates\n",
    "for item_id, item in ITEMS_BY_ID.items():\n",
    "    if not item[\"website\"]:\n",
    "        continue\n",
    "    if item[\"website\"] in websites and websites[item[\"website\"]] != item[\"primaryTopic\"]:\n",
    "        #print(item[\"website\"], \"has at least 2 diff topics:\", item[\"primaryTopic\"], websites[item[\"website\"]])\n",
    "        websites_w_mx_topics.add(item[\"website\"])\n",
    "    websites[item[\"website\"]] = item[\"primaryTopic\"]\n",
    "    \n",
    "print(\"{}% of unique websites in entries having websites\".format(round(len(websites)/len([v for v in ITEMS_BY_ID.values() if v[\"website\"]]), 2)))\n",
    "print(\"{}% of entries having websites that have different primary topic in the two different entries\".format(round(len(websites_w_mx_topics)/len(websites), 2))) # few have multiple no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_deduped = dict()\n",
    "for item_id, item in ITEMS_BY_ID.items():\n",
    "    key = item[\"website\"] if item[\"website\"] else item_id\n",
    "    hits_deduped[key] = hits_deduped.get(key, 0) + cumulative_values[\"browse\"].get(item_id, 0)\n",
    "\n",
    "item_hits = []\n",
    "zero = 0\n",
    "for website, hits in hits_deduped.items():\n",
    "    if hits:\n",
    "        item_hits.append(dict(value=hits))\n",
    "    else:\n",
    "        zero += 1\n",
    "        \n",
    "chart = alt.Chart(pd.DataFrame(item_hits), width=800, title=\"From 2022-09 until 2024-03; deduplicated based on website URL. {}% entries not represented (no hits) of {} total\".format(\n",
    "    round(zero/len(hits_deduped) * 100), len(hits_deduped))\n",
    "                 ).mark_bar().encode(\n",
    "    alt.X('value', bin=alt.Bin(maxbins=100)),\n",
    "    y=alt.Y('count()', title='Number of entries with this many hits')\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits by primary topic\n",
    "hits_by_topic = dict()\n",
    "notopic = 0\n",
    "for item_id, item in ITEMS_BY_ID.items():\n",
    "    if not item[\"primaryTopic\"]:\n",
    "        notopic += 1\n",
    "        continue\n",
    "    hits_by_topic[item[\"primaryTopic\"]] = hits_by_topic.get(item[\"primaryTopic\"], dict(\n",
    "        hits=0, entries=0, entries_nonprimary=0\n",
    "    ))\n",
    "    hits_by_topic[item[\"primaryTopic\"]][\"hits\"] += cumulative_values[\"browse\"].get(item_id, 0)\n",
    "    hits_by_topic[item[\"primaryTopic\"]][\"entries\"] += 1 # primary\n",
    "\n",
    "print(\"NO TOPIC IN %s ENTRIES\"%notopic)\n",
    "\n",
    "for topic, v in hits_by_topic.items():\n",
    "    for item_id, item in ITEMS_BY_ID.items():\n",
    "        if item[\"primaryTopic\"] == topic:\n",
    "            continue\n",
    "        for tag in item[\"tags\"]:\n",
    "            assert tag == tag.lower()\n",
    "            assert topic == topic.lower()\n",
    "            if topic in tag:\n",
    "                v[\"entries_nonprimary\"] += 1\n",
    "                break\n",
    "    \n",
    "    print(\"Topic: %s Primary: %s Non-primary: %s\"%(topic, v[\"entries\"], v[\"entries_nonprimary\"]))\n",
    "\n",
    "vega_data = []\n",
    "for k, v in hits_by_topic.items():\n",
    "    vega_data.append(dict(topic=k,\n",
    "                          hits=v[\"hits\"],\n",
    "                          entries=v[\"entries\"]+v[\"entries_nonprimary\"],\n",
    "                          entries_primary=v[\"entries\"],\n",
    "                          entries_nonprimary=v[\"entries_nonprimary\"]))\n",
    "    \n",
    "\n",
    "alt.vconcat(alt.Chart(pd.DataFrame([dict(topic=k, hits=v[\"hits\"]) for k, v in hits_by_topic.items()]),\n",
    "                  width=800, title=\"Topics by current visibility\").mark_bar().encode(\n",
    "    x = alt.X('topic:N', title='Primary Topic', sort='-y'),\n",
    "    y=alt.Y('hits:Q', title='Hits')\n",
    "),\n",
    "alt.Chart(pd.DataFrame([dict(topic=k, hits=v[\"entries\"]) for k, v in hits_by_topic.items()]),\n",
    "                  width=800, title=\"Topics by current availability of organizations\").mark_bar().encode(\n",
    "    x = alt.X('topic:N', title='Primary Topic', sort='-y'),\n",
    "    y=alt.Y('hits:Q', title='Hits')\n",
    "))\n",
    "\n",
    "\n",
    "base = alt.Chart(pd.DataFrame(vega_data), title=\"Current availability and visibility of entries by topic\").encode(\n",
    "    x=alt.X('topic:N', sort='-y')\n",
    ")\n",
    "line =  base.mark_square(color='red').encode(y='hits:Q')\n",
    "bar = base.mark_bar(color='lightgray').encode(y='entries:Q')\n",
    "bar2 = base.mark_bar(color='lightgray').encode(y='entries_nonprimary:Q')\n",
    "bar3 = base.mark_bar(color='lightgray').encode(y='entries_primary:Q')\n",
    "\n",
    "alt.hconcat(\n",
    "    alt.layer(bar, line).resolve_scale(\n",
    "        y='independent'\n",
    "    ),\n",
    "    alt.layer(bar2, line).resolve_scale(\n",
    "    y='independent'\n",
    "),\n",
    "    alt.layer(bar3, line).resolve_scale(\n",
    "    y='independent'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by opportunity for visibility - ie, where hits less than availability\n",
    "\n",
    "for k, v in hits_by_topic.items():\n",
    "    v[\"opportunity\"] = (v[\"entries\"]+v[\"entries_nonprimary\"]) / v[\"hits\"]\n",
    "\n",
    "for topic, v in dict(sorted(hits_by_topic.items(), key=lambda item: item[1][\"opportunity\"], reverse=True)).items():\n",
    "    # having minimum 100 entries\n",
    "    if v[\"entries\"]+v[\"entries_nonprimary\"] >= 100:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arts\n",
    "sports\n",
    "recreation\n",
    "health\n",
    "kindergarden\n",
    "education\n",
    "self_help\n",
    "volunteer_work\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pca_(tags_list, n_components): # expects 1 flat list of strings\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(tags_list)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X.toarray())\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_normalized)\n",
    "\n",
    "    components = pca.components_\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        sorted_indices = np.argsort(component)[::-1]\n",
    "\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    return sum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. breaking up topics that have high opportunity\n",
    "# 2. and then cluster the rest - the bags of words are per topic\n",
    "for topic, v in hits_by_topic.items():\n",
    "    \n",
    "    tags_list = []\n",
    "    for item_id, item in ITEMS_BY_ID.items():\n",
    "        \n",
    "        in_this_topic = False\n",
    "        if item[\"primaryTopic\"] == topic:\n",
    "            in_this_topic = True\n",
    "            \n",
    "        for tag in item[\"tags\"]:\n",
    "            if topic in tag:\n",
    "                in_this_topic = True\n",
    "                break\n",
    "                \n",
    "        if in_this_topic:\n",
    "            tags_list.append(\" \".join(item[\"tags\"]))\n",
    "    print(topic, pca_(tags_list, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "filt = 0\n",
    "for term, hits in cumulative_values[\"search\"].items():\n",
    "    if \"hous\" in term.lower() or \"wohn\" in term.lower():\n",
    "    #if \"kinder\" in term.lower() or \"kita\" in term.lower():\n",
    "        filt += hits\n",
    "    tot += hits\n",
    "\n",
    "filt/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_letter = dict()\n",
    "\n",
    "for itemid, hits in cumulative_values[\"browse\"].items():\n",
    "    if ITEMS_BY_ID.get(itemid):\n",
    "        key = (ITEMS_BY_ID.get(itemid)[\"title\"].lower().strip())[0]\n",
    "        data_by_letter[key] = data_by_letter.get(key, dict(hits=0, entries=0))\n",
    "        data_by_letter[key][\"hits\"] += hits\n",
    "\n",
    "for item in ITEMS_BY_ID.values():\n",
    "    key = item[\"title\"].lower().strip()[0]\n",
    "    data_by_letter[key] = data_by_letter.get(key, dict(hits=0, entries=0))\n",
    "    data_by_letter[key][\"entries\"] += hits\n",
    "\n",
    "tot_hits = sum([v[\"hits\"] for v in data_by_letter.values()])\n",
    "tot_entries = sum([v[\"entries\"] for v in data_by_letter.values()])\n",
    "\n",
    "vega_letter_data = []\n",
    "for k, v in data_by_letter.items():\n",
    "    vega_letter_data.append(dict(\n",
    "        letter=k,\n",
    "        measure=\"entries (proportion of total)\",\n",
    "        count=v[\"entries\"]/max_entries\n",
    "    ))\n",
    "    vega_letter_data.append(dict(\n",
    "        letter=k,\n",
    "        measure=\"hits (proportion of total)\",\n",
    "        count=v[\"hits\"]/max_hits\n",
    "    ))\n",
    "\n",
    "print(data_by_letter[\"a\"][\"hits\"])\n",
    "print(data_by_letter[\"a\"][\"hits\"]/tot_hits)\n",
    "print(data_by_letter[\"a\"][\"entries\"]/tot_entries)\n",
    "\n",
    "alt.Chart(pd.DataFrame(vega_letter_data),\n",
    "                  width=800, title=\"Alphabetic sort strongly affects visibility\").mark_line().encode(\n",
    "    x=alt.X('letter:O'),\n",
    "    y=alt.Y('count:Q'),\n",
    "    color='measure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim reduce based on categories - not quite right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texts_to_cluster = []\n",
    "item_ids = []\n",
    "for item in ITEMS_BY_ID.values():\n",
    "    #if \"de\" not in item[\"brief\"]:\n",
    "    #    print(item)\n",
    "    #break\n",
    "    # item[\"brief\"][\"en\"].replace(\"[DeepL:] \", \"\")\n",
    "    # vre.sub(r'\\d+', '', text)\n",
    "    texts_to_cluster.append(re.sub(r'\\d+', '', (item[\"brief\"].get(\"de\", \"\") + \" \" + item[\"brief\"].get(\"en\", \"\").replace(\"[DeepL:] \", \"\") +\\\n",
    "                            \" \" + item[\"title\"] +\\\n",
    "                            \" \".join(\"tags\")+ \" \" + (item[\"primaryTopic\"] if item[\"primaryTopic\"] else \"\"))))\n",
    "    item_ids.append(item[\"id\"])\n",
    "\n",
    "len(texts_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "combined_stop_words = list(text.ENGLISH_STOP_WORDS.union(german_stop_words))\n",
    "print(\"STOP WORDS COMBO:\", len(combined_stop_words))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=combined_stop_words)\n",
    "X = vectorizer.fit_transform(texts_to_cluster)\n",
    "\n",
    "num_clusters = 15\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "cluster_combos = []\n",
    "for i in range(num_clusters):\n",
    "    cluster_indices = np.where(clusters == i)[0]\n",
    "    cluster_combos.append(\" \".join([texts_to_cluster[idx] for idx in cluster_indices]))\n",
    "\n",
    "    X_cluster = vectorizer.fit_transform([texts_to_cluster[idx] for idx in cluster_indices])\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    average_tfidf_scores = np.mean(X_cluster.toarray(), axis=0)\n",
    "\n",
    "    sorted_indices = np.argsort(average_tfidf_scores)[::-1]\n",
    "\n",
    "    top_words_indices = sorted_indices[:7]\n",
    "    top_words = [feature_names[idx] for idx in top_words_indices]\n",
    "\n",
    "    label = \" \".join(top_words)\n",
    "\n",
    "    print(i + 1,\",\",len(cluster_indices),\",\", label)\n",
    "    for idx in cluster_indices:\n",
    "        ITEMS_BY_ID[item_ids[idx]][\"Cluster_ID\"] = i+1\n",
    "        ITEMS_BY_ID[item_ids[idx]][\"Cluster_Words\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"id , title , Cluster_ID, Cluster_Words\")\n",
    "for item in ITEMS_BY_ID.values():\n",
    "    print(item[\"id\"], \",\", item[\"title\"].replace(\",\", \" \"), \",\", item[\"Cluster_ID\"], \",\", item[\"Cluster_Words\"], \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vega_values_data = []\n",
    "\n",
    "for jo in json_objects:\n",
    "    timeframe, values, summary = process_requesturls_file(jo, cumulative_values)\n",
    "    print(values)\n",
    "    for item_id, hits in values.items():\n",
    "        if not ITEMS_BY_ID.get(item_id):\n",
    "            continue\n",
    "        if ITEMS_BY_ID\n",
    "    break\n",
    "    vega_values_data.append(dict(\n",
    "        month=timeframe,\n",
    "        user_journey=\"search\" if summary[\"search\"] > 0 else \"browse\",\n",
    "        hits=summary[\"search\"] if summary[\"search\"] > 0 else summary[\"browse\"],\n",
    "    ))\n",
    "    if summary[\"browse\"] > 0:\n",
    "        vega_summary_data.append(dict(\n",
    "            month=timeframe,\n",
    "            user_journey=\"suggest\",\n",
    "            hits=summary[\"suggest\"]\n",
    "        ))\n",
    "    print(timeframe, summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits by primary topic\n",
    "hits_by_topic = dict()\n",
    "notopic = 0\n",
    "for item_id, item in ITEMS_BY_ID.items():\n",
    "    if not item[\"primaryTopic\"]:\n",
    "        notopic += 1\n",
    "        continue\n",
    "    hits_by_topic[item[\"primaryTopic\"]] = hits_by_topic.get(item[\"primaryTopic\"], dict(hits=0, entries=0))\n",
    "    hits_by_topic[item[\"primaryTopic\"]][\"hits\"] += cumulative_values[\"browse\"].get(item_id, 0)\n",
    "    hits_by_topic[item[\"primaryTopic\"]][\"entries\"] += 1\n",
    "\n",
    "vega_data = []\n",
    "for k, v in hits_by_topic.items():\n",
    "    vega_data.append(dict(topic=k, count=v[\"hits\"], measure=\"hits\"))\n",
    "    vega_data.append(dict(topic=k, count=v[\"entries\"], measure=\"entries\"))\n",
    "    \n",
    "print(\"NO TOPIC IN %s ENTRIES\"%notopic)\n",
    "\n",
    "\n",
    "\n",
    "chart = alt.Chart(pd.DataFrame(vega_data),\n",
    "                  width=800, title=\"Topics by current availability and visibility\").mark_bar().encode(\n",
    "    x = alt.X('topic:N', title='Primary Topic', sort='-y'),\n",
    "    y=alt.Y('count:Q'),\n",
    "    row='measure'\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = data.wheat()\n",
    "\n",
    "base = alt.Chart(source).encode(x='year:O')\n",
    "\n",
    "bar = base.mark_bar().encode(y='wheat:Q')\n",
    "\n",
    "line =  base.mark_line(color='red').encode(\n",
    "    y='wages:Q'\n",
    ")\n",
    "\n",
    "(bar + line).properties(width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_objects[1][\"general\"][\"start_date\"])\n",
    "print(json_objects[1][\"general\"][\"end_date\"])\n",
    "\n",
    "for r in json_objects[1][\"requests\"][\"data\"]:\n",
    "    \n",
    "    if r[\"data\"].startswith(\"/search/\"):\n",
    "        searchterm = r[\"data\"][len(\"/search/\"):]\n",
    "        print(r[\"hits\"][\"count\"], r[\"visitors\"][\"count\"], searchterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"ClusteredItems.json\", 'w') as json_file:\n",
    "    json.dump(ITEMS_BY_ID, json_file, indent=4)\n",
    "    \n",
    "with open(\"CumulativeHitsData.json\", 'w') as json_file:\n",
    "    json.dump(cumulative_values, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
